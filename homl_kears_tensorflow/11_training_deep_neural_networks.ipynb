{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Deep Neural Networks\n",
    "This notebook presents problems encountered when training deep neural networks and some of the\n",
    "techniques that may be used to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Index\n",
    "\n",
    "[Glorot and He Initialization](#Glorot-and-He-Initialization)\n",
    "\n",
    "[Nonsaturating Activation Functions](#Nonsaturating-Activation-Functions)\n",
    "\n",
    "[Batch Normalization](#Batch-Normalization)\n",
    "\n",
    "[Faster Optimizers](#Faster-Optimizers)\n",
    "\n",
    "[Learning Rate Scheduling](Learning-Rate-Scheduling)\n",
    "\n",
    "[Avoiding Overfitting Through Regularization](#Avoiding-Overfitting-ThroughRegularization)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Glorot and He Initialization\n",
    "Glorot and Bengio proposed a way to alleviate the unstable gradient problem. The signal needs to\n",
    "flow properly in the forward direction while making predictions and in the reverse direction when\n",
    " backpropagating gradients.\n",
    "\n",
    "By default Keras uses the Glorot initialization with a uniform distribution. When creating a\n",
    "layer this, it can be changed to the He initialization by setting the\n",
    "```kernel_initializer=\"he_uniform\"``` or ```kernel_initializer=\"he_normal\"```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.layers.core.Dense at 0x1d566573e88>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# He normal\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the He initialization is to be used with a uniform distribution based on $fan_{avg}$ rather\n",
    "than $fan_{in}$, the ```VarianceScaling``` initializer should be used"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.layers.core.Dense at 0x1d56dbcc388>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VarianceScaling\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2.0, mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nonsaturating Activation Functions\n",
    "*Leaky ReLU* can be used to prevent dying neurons. Dying neurons occur when its weights get\n",
    "tweaked in such a way that the weighted sum of its inputs are negative for all instances in the\n",
    "training set. When this happens, it keeps on outputting zeros and Gradient Descent has no impact.\n",
    " Leaky ReLU fixes this as the activation has a small slope (below x = 0) that ensures that the\n",
    " leaky ReLU never dies.\n",
    "\n",
    " To use the leaky ReLU activation function, a ```LeakyReLU``` layer needs to be added to the\n",
    " model after the layer that it should be applied to. Other activation functions such as variants\n",
    " to the ```LeakyReLU``` such as the ```Parametric Leaky ReLU``` and ```Scaled ELU``` a variant of\n",
    "  the *Exponential Linear Unit (ELU)* are shown below"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Leaky ReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Parametric Leaky ReLU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU(),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# SELU\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Normalization\n",
    "Batch normalization is a technique for training very deep neural networks that standardizes the\n",
    "inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process\n",
    "and  dramatically reducing the number of training epochs required to train deep networks.\n",
    "\n",
    "The technique adds an operation to the model before or after the activation function of each\n",
    "hidden layer. The operation zero-centers and normalizes each input, then scales and shifts the\n",
    "result using two new parameter vectors per layer; one for scaling and the other for shifting. The\n",
    " operation lets the model learn the optimal scale and mean of each of the layers inputs.\n",
    "\n",
    " A ```BatchNormalization``` layer should be added before or after each hidden layers activation\n",
    " function and optionally before the first layer in the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Batch Normalization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch Normalization creates four parameters $\\gamma, \\beta, \\mu$ and $\\sigma$.\n",
    "- $\\gamma$ is the output scale parameter vector for the layer (it contains one scale per input)\n",
    "- $\\beta$ is the output shift (offset) parameter vector for the layer (it contains one offset\n",
    "parameter per input). Each input is offset by its corresponding shift parameter\n",
    "- $\\mu$ is the vector of input means, evaluated over the whole mini-batch (contains one mean per\n",
    "input)\n",
    "- $\\sigma$ is the vector of input standard deviations, also evaluated over the whole mini-batch\n",
    "(it contains one standard deviation per input)\n",
    "\n",
    "We can take a look at the parameters of the first BN layer. Two are trainable (by\n",
    "backpropagation), and two are not"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[('batch_normalization/gamma:0', True),\n ('batch_normalization/beta:0', True),\n ('batch_normalization/moving_mean:0', False),\n ('batch_normalization/moving_variance:0', False)]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainable\n",
    "[(var.name, var.trainable) for var in model.layers[1].variables]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depending on the task it might be best to add the BN layers before the activation functions,\n",
    "rather than after. To do this the activation function must be removed from the hidden layers and\n",
    "then added separately after the BN layers. Since BN includes one offset parameter per input, the\n",
    "bias term should be removed from the previous layer (passing ```use_bias=False```)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# BN\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Faster Optimizers\n",
    "Training a large deep neural network can be very slow. Applying a good initialization strategy\n",
    "for the connection weights, using a good activation function, using Batch Normalization, and\n",
    "reusing parts of a different neural network trained on a similar task.\n",
    "\n",
    "Another huge speed boost comes from using a faster optimizer regular Gradient Descent!\n",
    "\n",
    "### Momentum Optimization\n",
    "*Momentum optimization* or SGD with momentum is method which helps accelerate\n",
    "gradients  vectors in the right directions, thus leading to faster converging."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Momentum Optimization\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nestrov Accelerated Gradient\n",
    "The *Nestrov Accelerated Gradient (NAG)* measures the gradient cost function not at the local\n",
    "position $\\theta$ but slightly ahead in the direction of the momentum, at $\\theta + \\beta m$.\n",
    "This small tweak works because the momentum vector will be pointing in the right direction\n",
    "(toward the optimum), so it will be slightly more accurate than the gradient at the original\n",
    "position.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# NAG\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RMSProp\n",
    "*RMPSProp* accumulates the gradients from the most recent iterations (as opposed to all the\n",
    "gradients from the beginning of training). It does so by using exponential decay in the first step\n",
    "\n",
    "The decay rate $\\rho$ (rho) is a hyperparameter so it can be tuned. The default value of 0.9\n",
    "tends to work well, however."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adam and Nadam Optimisation\n",
    "*Adam* or *adaptive moment estimation* combines the idea of momentum optimization and RMSProp:\n",
    "just like momentum, it keeps track of an exponentially decaying average of past gradients; and\n",
    "like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "Adam is and *adaptive learning rate* algorithm. it requires less tuning of the learning rate\n",
    "hyperparameter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Adam\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate Scheduling\n",
    "Finding a good learning rate is essential. If it is set too high, training may diverge. If it is\n",
    "set too low, training will eventually converge to the optimum, but it will take very long.\n",
    "\n",
    "The initial practice dealt with a constant learning rate. However, this can be improved a lot.\n",
    "For example, if you start with a large learning rate and then reduce it once training stops\n",
    "making fast progress, you can reach a good solution faster with the optimal constant learning\n",
    "rate. It can also be beneficial to start with a low learning rate, increase it, then drop it again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Power scheduling\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Exponential scheduling\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Piecewise scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return\n",
    "    else:\n",
    "        return 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Performance scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tf.keras allows the user to implement learning rate scheduling via ```keras.optimizers\n",
    ".schedules```, then pass this learning rate to the optimizer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Native keras\n",
    "s = 20 * len(range(100)) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avoiding Overfitting Through Regularization\n",
    "Deep neural networks tend to have thousands if not millions of parameters. This makes them\n",
    "extremely powerful and flexible but also prone to overfitting. Regularization techniques are\n",
    "required to ensure these models are able to generalize.\n",
    "\n",
    "### $\\ell_{1}$ and $\\ell_{2}$ Regularization\n",
    "The $\\ell_{2}$ regularizer is called at each step during training to compute the regularization\n",
    "loss. This is then added to the final loss. The $\\ell_{1}$ regularizer can be called through\n",
    "Keras by ```keras.regularizers.l1()```."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# L2 norm\n",
    "layer = keras.layers.Dense(100, activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the same regularizer should be applied to all the layers in the network Python's\n",
    "```functools.partial()``` may be used, which lets you create a thing wrapper callable, with some\n",
    "default arguments\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Wrapper\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, activation='elu', kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dropout\n",
    "*Dropout* is a successful regularization technique that is able to get a 1-2% boost in accuracy.\n",
    "It ia a fairly simple algo, at every training step, every neuron (including the input neurons but\n",
    " always excluding the output neurons) has a probability *p* of being temporarily *dropped out*,\n",
    " meaning it will be entirely ignored during this training step, but it may be active during the\n",
    " next step.\n",
    "\n",
    " A unique neural network is generated at each training step. Once there have been 10,000 training\n",
    "  step, there are 10,000 different neural networks. These neural networks are not independent\n",
    "  from each other, they shar some weights. The resulting neural network can be seen as an\n",
    "  averaging ensemble of all these smaller neural networks.\n",
    "\n",
    "To implement on keras the ```keras.layers.Dropout``` layer should be used. During training it\n",
    "will randomly drop some inputs (setting them to 0) and divides the remaining inputs by the keep\n",
    "probability. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Dropout\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Max-Norm Regularization\n",
    "For each neuron *max-norm regularization*, constraints the weights $w$ of the incoming\n",
    "connections such that $\\| x\\|_{2} \\leq r$, where $r$ is the max-norm hyperparameter and\n",
    "$\\|\\cdot \\|_{2}$ is the $\\ell_{2}$ norm.\n",
    "\n",
    "Reducing $r$ increases the amount of regularization and helps reduce overfitting. To implement\n",
    "max-norm regularization in Keras, set the ```kernel_constraint``` argument of each hidden layer\n",
    "to a ```max_norm()``` constraint with the appropriate max value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.layers.core.Dense at 0x1d55ef4a688>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max-norm regularization\n",
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}